---
title: "Automated Scientific Discovery"
subtitle: "Patterns for AI-assisted research and agent-driven experimentation"
slug: "automated-scientific-discovery"
---

import { Citation, CitationsList } from 'gatsby-citation-manager'

## A Survey, Not “A Full Scientist in a Box”

This page is a catalog of patterns—not a single monolithic “automated science” demo.

Many writeups about automated scientific discovery try to tell one complete story: the system reads papers, generates hypotheses, writes code, runs experiments, makes plots, writes a paper, and submits it. That can be inspiring, but it often hides the real value: the reusable building blocks you can apply to your own research and AI engineering work.

So instead of constructing one giant end‑to‑end example, we’ll survey the patterns that show up repeatedly in the research and in practical systems—and outline how you would structure each pattern in Tactus using small snippets and pseudocode.

If you’re new to the mental model that motivates these patterns, start here:

- [Why a New Language?](/why-new-language/)
- [Guardrails](/guardrails/)

## The Landscape: A Spectrum of Autonomy

Automated discovery frameworks tend to cluster into a few families. The important distinction is not “how smart is the model,” but “where does the control flow live?”—who decides what happens next?

| Family | What it optimizes for | Typical patterns |
|---|---|---|
| Research assistants | Fast, grounded information work | Retrieval, summarization, citation, question decomposition |
| Hybrid discovery workflows | Human‑led discovery with automation “muscle” | Human chooses direction; agents expand, test, and summarize |
| Autonomous discovery pipelines | Push the full loop end‑to‑end | Hypothesis generation, experiment search, critique/repair, write‑up |

Recent work explores all three.

Systems like *The AI Scientist* <Citation data={{ type: "article-journal", title: "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery", author: [{ family: "Lu", given: "Chris" }, { family: "Lu", given: "Cong" }, { family: "Lange", given: "Robert Tjarko" }, { family: "Foerster", given: "Jakob" }, { family: "Clune", given: "Jeff" }, { family: "Ha", given: "David" }], issued: { 'date-parts': [[2024, 8]] }, URL: "https://arxiv.org/abs/2408.06292" }} /> and *AI Scientist v2* <Citation data={{ type: "article-journal", title: "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search", author: [{ family: "Yamada", given: "Yutaro" }, { family: "Lange", given: "Robert Tjarko" }, { family: "Lu", given: "Cong" }, { family: "Hu", given: "Shengran" }, { family: "Lu", given: "Chris" }, { family: "Foerster", given: "Jakob" }, { family: "Clune", given: "Jeff" }, { family: "Ha", given: "David" }], issued: { 'date-parts': [[2025, 4]] }, URL: "https://arxiv.org/abs/2504.08066" }} /> explore end‑to‑end autonomy (hypothesis → experiments → analysis → write‑up). Meanwhile, research assistants like OpenResearcher <Citation data={{ type: "article-journal", title: "OpenResearcher: Unleashing AI for Accelerated Scientific Research", issued: { 'date-parts': [[2024, 8]] }, URL: "https://arxiv.org/abs/2408.06941" }} /> focus on grounding, retrieval, and synthesis to accelerate human‑led research.

Tactus sits underneath all of these styles. It’s designed to help you write *procedures* with explicit tool access, guardrails, durable checkpoints, and evaluation—because once you move past demos, those are the problems you spend your time on.

## What Changes When You Yield Control Flow to the Agent

Most classical software engineering assumes that your `if/then` logic governs the program’s flow. But in agentic systems, the model often decides:

- which tool to call,
- how many times to call it,
- when to stop,
- and which branch to take next.

That’s the point of patterns like ReAct: you explicitly give the agent control over control flow. It’s a new kind of program.

The responsibility question follows immediately: how can you be responsible for a procedure that begins, ends, and completes reliably if the flow is not dictated by deterministic branching logic?

This is where “guardrails” stop being optional. A system that can call tools must also have enforceable boundaries: limits on steps, time, cost, tool access, data access, and side effects. Otherwise you can’t safely run the loop unattended—and you can’t scale it to many trials.

This is also why “automated discovery” is not one feature. It’s a collection of patterns for designing, constraining, and evaluating agentic control flow.

## A Catalog of Patterns (and How They Map to Tactus)

Below is a set of patterns that recur across automated discovery papers and systems. Each one is useful on its own. You don’t need all of them to get value.

### 1) Literature Triage (Search → Filter → Summarize → Organize)

**What it is:** A pipeline that turns “too many papers” into a small curated working set.

**Where it shows up:** Most research assistants (RAG‑style or tool‑based) and most autonomous systems (as a novelty check and background builder).

**How to structure it in Tactus:** Treat retrieval and summarization as tools. Keep tool access staged and auditable. Persist intermediate state so the “research thread” survives interruptions.

```tactus
LiteratureTriage = Procedure {
  input  = { query = string, max_papers = number }
  output = { shortlist = array, notes = array }

  function(input)
    local hits = Step.checkpoint(function()
      return SearchPapers({query = input.query, k = 50})
    end)
    local papers = Step.checkpoint(function()
      return FetchPapers({ids = hits[1:input.max_papers]})
    end)
    local notes = SummarizeMany(papers)
    local shortlist = Human.select({items = notes, k = 8})
    return { shortlist = shortlist, notes = notes }
  end
}
```

The key idea is procedural: you’re not “writing the science,” you’re building a dependable process for information work.

Note the use of `Step.checkpoint(...)`: even if the procedure is replayed (after an approval gate, a restart, or a crash), expensive retrieval steps can return cached results instead of re-running blindly.

### 2) Hypotheses as Data (and Preregistered Success Criteria)

**What it is:** Hypothesis generation is easy. The hard part is making hypotheses testable—and deciding what counts as success *before* you run expensive experiments.

**How to structure it in Tactus:** Make hypotheses structured outputs, not prose. Pair each hypothesis with an evaluation plan (metrics, thresholds, datasets).

```text
hypothesis = {
  claim: "Reducing tool access in the first stage lowers misbehavior rate",
  variables: ["tools_per_stage", "approval_gate"],
  metric: "misbehavior_rate",
  target: "<= 2%",
  dataset: "eval_v2"
}
```

This forces clarity: “what would change my mind?” becomes part of the artifact.

### 3) Experiment Search (Sweeps, Tree Search, Bandits)

**What it is:** Once you have a hypothesis, you need a search strategy over configurations. AI Scientist v2 popularized “agentic tree search” as one approach to branching exploration <Citation data={{ type: "article-journal", title: "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search", author: [{ family: "Yamada", given: "Yutaro" }, { family: "Lange", given: "Robert Tjarko" }, { family: "Lu", given: "Cong" }, { family: "Hu", given: "Shengran" }, { family: "Lu", given: "Chris" }, { family: "Foerster", given: "Jakob" }, { family: "Clune", given: "Jeff" }, { family: "Ha", given: "David" }], issued: { 'date-parts': [[2025, 4]] }, URL: "https://arxiv.org/abs/2504.08066" }} />.

**How to structure it in Tactus:** Make the search explicit. Enforce budgets and stopping conditions. Persist a “tree of attempts” as durable state.

```text
node = {
  id: "n42",
  parent_id: "n19",
  config: { model: "gpt-4o-mini", temperature: 0.2, tools: ["search"] },
  status: "completed",
  metrics: { pass_rate: 0.91, cost_usd: 0.06 }
}
```

You don’t need the entire algorithm in one procedure. The important part is the data model: experiments are nodes with provenance.

If you want to run branches in parallel, the shape is usually: spawn a handful of experiment nodes, wait for completion, then decide what to expand next. In Tactus terms, this is `Procedure.spawn(...)` + `Procedure.wait_any(...)` / `Procedure.wait_all(...)`.

```tactus
handles = {}
for _, config in ipairs(frontier) do
  table.insert(handles, Procedure.spawn("run_experiment_node", {config = config}))
end

local done = Procedure.wait_any(handles)
local result = Procedure.wait(done)
```

That same structure supports “replication nodes” (run the same config with different seeds), “ablation nodes” (remove one component), and “aggregation nodes” (combine metrics into one decision).

### 4) Safe Execution (Sandboxing and Narrow Tool Interfaces)

**What it is:** If the workflow generates code, runs simulations, or touches files, then safety is not an afterthought. “Run arbitrary code” is a privilege boundary.

**How to structure it in Tactus:** Provide a narrow tool such as `RunExperiment(code, limits)` that executes in an isolated environment (e.g., a container) and returns artifacts without exposing credentials or broad host access.

```tactus
result = RunExperiment({
  code = generated_python,
  cpu = 2,
  memory_gb = 4,
  timeout_s = 600,
})
```

### 5) Critique Loops (Repair, Not Just Generate)

**What it is:** Most robust systems aren’t one‑shot. They generate, then critique, then repair. Critique can be an LLM, a VLM on plots, or deterministic validators.

**How to structure it in Tactus:** Make critique a step with clear inputs/outputs. Keep the repair loop bounded and reviewable.

```text
for attempt in 1..MAX_REPAIRS:
  critique = Critique(artifact, rubric)
  if critique.ok: break
  artifact = Repair(artifact, critique)
```

The goal is not “the model will fix it.” The goal is “we built a bounded loop whose failures are visible.”

Two practical refinements show up repeatedly in robust agentic loops:

- **Hard caps** on attempts (so the loop always ends).
- **Per-turn tool control** (so the agent only has dangerous tools when the procedure is ready for them).

In Tactus, callable agent syntax supports per-call tool overrides (including an empty tool list), which makes it easy to prevent “tool drift” during critique, summarization, and write-up turns.

```tactus
reviewer({ message = "Summarize the result", tools = {} })
reviewer({ tools = {search, done} }) -- enable only what’s needed this turn
```

### 6.5) Specs vs Evals (Invariants vs Reliability)

As soon as you scale experimentation, two different kinds of questions matter:

- **Specs**: “Does this ever do something forbidden?” (invariants)
- **Evals**: “How often does it work across real inputs?” (reliability)

Both are “guardrails,” but they fail differently. Specs fail loudly and quickly. Evals reveal brittleness and drift.

In Tactus terms, specs tend to encode policies (“never call the side‑effect tool without approval”), while evals measure outcomes over datasets (often with multiple runs to estimate reliability).

### 6) Reliability, Not One‑Off Scores

Single‑run success is not science. For LLMs, reliability is distributional: the same input can produce different outputs. Benchmark efforts like MASSW emphasize evaluating AI workflows and reliability across tasks <Citation data={{ type: "article-journal", title: "MASSW: A New Dataset and Benchmark Tasks for AI-Assisted Scientific Workflows", author: [{ family: "Zhang", given: "Xingjian" }, { family: "Xie", given: "Yutong" }, { family: "Huang", given: "Jin" }, { family: "Ma", given: "Jinge" }, { family: "Pan", given: "Zhaoying" }, { family: "Liu", given: "Qijia" }, { family: "Mei", given: "Qiaozhu" }], issued: { 'date-parts': [[2024, 6]] }, URL: "https://arxiv.org/abs/2406.06357" }} />.

**How to structure it in Tactus:** Use repeated trials, aggregated metrics, and explicit acceptance criteria.

```text
pass_rate = Evaluate(procedure = candidate, dataset = heldout, runs = 50)
if pass_rate >= 0.95: accept()
```

### 7) Tradeoffs and Pareto Frontiers (Quality vs Cost vs Latency)

Discovery workflows always have tradeoffs. The “best” configuration depends on constraints: budget, latency, failure tolerance, and stakes.

**How to structure it in Tactus:** Treat cost/latency as measurements alongside quality. Choose configurations on a Pareto frontier instead of optimizing a single scalar.

```text
metrics = { score: 0.91, cost_usd: 0.08, p95_ms: 1200 }
```

### 8) Human Oversight (Outer‑Loop Governance)

The most practical automation pattern is: let the agent explore, but require humans for irreversible decisions.

Examples:

- approve high‑cost branches,
- approve publication claims,
- approve tool expansions,
- approve data access changes.

In Tactus terms, these are durable checkpoints where a human can intervene without keeping the whole workflow “live” for hours.

### 9) Provenance (The Lab Notebook That Writes Itself)

In discovery, the outputs aren’t just “the best result.” They’re the trail:

- what was tried,
- why it was tried,
- what failed,
- what improved,
- and what evidence supports the claim.

**How to structure it in Tactus:** Make the procedure produce a structured experiment log artifact. This is what turns a pile of runs into something auditable.

```text
return {
  best_config,
  experiment_log,
  conclusions
}
```

## Practical Advice: How to Start

If you want to apply these ideas to your own work, start with a small inner loop and scale outward:

1. Pick one measurable outcome (quality score, reliability, latency, cost).
2. Define an evaluation dataset and acceptance criteria.
3. Run repeated trials and aggregate results.
4. Add a search strategy (sweep, tree, bandit).
5. Add guardrails (budget/time/step limits + staged tools).
6. Add human gates only where you need them (expensive, irreversible, or publishable).

If you only adopt one idea from the research literature, make it this: treat *the procedure* as the unit of experimentation, and treat guardrails/specs/evals as first‑class parts of that procedure—not bolted-on conventions.

## Conclusion

Automated scientific discovery is not one “system” you either have or don’t have. It’s a set of patterns you can compose:

- grounded information workflows,
- structured hypotheses,
- systematic search,
- safe execution boundaries,
- critique and repair,
- reliability evaluation,
- cost/quality tradeoffs,
- human governance,
- and provenance.

As agents take on more control flow, these concerns become the program itself. The core promise of Tactus is that it gives you a place to express those concerns directly—so your “code” matches what you actually have to reason about in an agentic discovery loop.

The goal isn’t to remove humans from science. It’s to remove waste: repeated manual setup, lost progress, unmeasured changes, and untraceable decisions. That’s what turns experimentation into discovery.

## References

<CitationsList />
